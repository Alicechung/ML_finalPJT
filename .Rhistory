mix_multi<- function(X, k, tol, seed){
library(MCMCpack)
set.seed(999)
pis<- rdirichlet(1, alpha = rep(100, k))
thetas<- matrix(NA, nrow=k, ncol=ncol(X))
for(z in 1:k){
thetas[z,]<- rdirichlet(1, alpha=rep(100, ncol(X)))
}
rs<- matrix(NA, nrow=nrow(X),ncol=k)
a<- 0
t<- 1
e.log<- function(X, pis, thetas, rs){
log.pis<- log(pis)
log.thetas<- log(thetas)
score<- 0
for(z in 1:nrow(X)){
part1<- rs[z,]*log.pis
part2<- 0
for(j in 1:k){
part2<- part2 + sum(rs[z,j]*X[z,]*log(thetas[j,] + .000001))
}
score<- score + sum(part1) + part2
}
return(score)
}
while(a==0){
for(i in 1:nrow(X)){
for(j in 1:k){
denom<- thetas[j,]^{-X[i,]}
nums<- thetas[-j,]
new_num<- 0
for(l in 1:nrow(nums)){
new_num<- new_num + (pis[l]/pis[j])*prod(nums[l,]^{X[i,]}*denom)}
rs[i,j]<- ifelse(is.na(1/(1 + new_num))==F,1/(1 + new_num), 0)
}
}
e.old<- e.log(X, pis,thetas,  rs)
thetas<- t(rs)%*%X
for(z in 1:k){
thetas[z,]<- (thetas[z,] )/(sum(thetas[z,] ) )
}
pis<- apply(rs, 2, sum)/sum(rs)
t<- t + 1
if(t>1){
e.new<- e.log(X, pis, thetas, rs)
change<- e.new - e.old
print(abs(change))
if(abs(change)<tol){
a<- 1}
}
}
out<- list(thetas, pis, rs)
names(out)<- c('thetas', 'pis', 'rs')
return(out)
}
k<- 6
test<- mix_multi(senate_matrix_12, k, 1e-5, 999)
table(apply(test$rs, 1, which.max), k_cluster_6$cluster) # confusion matrix
mult_words<- matrix(NA, nrow = k, ncol=10)
for(z in 1:k){
diff<- test$thetas[z,] - apply(test$thetas[-z,], 2, mean)
mult_words[z,]<- colnames(senate_matrix_12)[order(diff, decreasing=T)[1:10]]
}
t(mult_words)
senate <- as_tibble(read.csv("C:/Users/minju/Dropbox/UChicago/MachineLearning/HW2/final_dtm.csv"))
shelby <- as_tibble(read.csv("C:/Users/minju/Dropbox/UChicago/MachineLearning/HW2/hw2_sh.csv"))
sessions <- as_tibble(read.csv("C:/Users/minju/Dropbox/UChicago/MachineLearning/HW2/hw2_ss.csv"))
View(shelby)
# Shelby
shelbym <- mean(shelby$nfire, shelby$nimmi, shelby$nnomin)
# Shelby
summary(shelby)
# Shelby
mean(shelby)
# Shelby
summary(shelby)
summary(sessions)
# Shelby
summary(shelby)
summary(sessions)
k_cluster_6$cluster[k_cluster_6$cluster == 1]
k_cluster_6$cluster[k_cluster_6$cluster == 1]
k_cluster_6$cluster[k_cluster_6$cluster == 2]
print(length(k_cluster_6$cluster[k_cluster_6$cluster == 1]))
k_cluster_6$cluster[k_cluster_6$cluster == 2]
print(length(k_cluster_6$cluster[k_cluster_6$cluster == 1]))
print(k_cluster_6$cluster[k_cluster_6$cluster == 2])
print(length(k_cluster_6$cluster[k_cluster_6$cluster == 1]))
(k_cluster_6$cluster[k_cluster_6$cluster == 2])
print(length(k_cluster_6$cluster[k_cluster_6$cluster == 1]))
k_cluster_6$cluster[k_cluster_6$cluster == 2]
View(senate)
k_cluster_6$cluster
senate <- read.csv("C:/Users/minju/Dropbox/UChicago/MachineLearning/HW2/final_dtm.csv")
shelby <- read.csv("C:/Users/minju/Dropbox/UChicago/MachineLearning/HW2/hw2_sh.csv")
sessions <- read.csv("C:/Users/minju/Dropbox/UChicago/MachineLearning/HW2/hw2_ss.csv")
# Shelby
summary(shelby)
summary(sessions)
##Shelby uses the term "fire department" more often than Sessions. Sessions uses the terms "immigration" and "nomination" more often than Shelby.
senate_matrix <- as.matrix(senate)
# Matix without the first and second columns
senate_matrix_12 <- senate %>%
select(-(1:2)) %>%
as.matrix()
View(senate)
senate <- as_tibble(read.csv("C:/Users/minju/Dropbox/UChicago/MachineLearning/HW2/final_dtm.csv"))
shelby <- as_tibble(read.csv("C:/Users/minju/Dropbox/UChicago/MachineLearning/HW2/hw2_sh.csv"))
sessions <- as_tibble(read.csv("C:/Users/minju/Dropbox/UChicago/MachineLearning/HW2/hw2_ss.csv"))
View(senate)
# Shelby
summary(shelby)
summary(sessions)
##Shelby uses the term "fire department" more often than Sessions. Sessions uses the terms "immigration" and "nomination" more often than Shelby.
senate_matrix <- as.matrix(senate)
# Matix without the first and second columns
senate_matrix_12 <- senate %>%
select(-(1:2)) %>%
as.matrix()
senate_matrix <- as.matrix(senate)
# Matix without the first and second columns
senate_matrix_12 <- senate %>%
select(-(1:2)) %>% as.matrix()
library(tidyverse)
library(ggplot2)
library(tidyr)
library(readr)
library(purrr)
library(dplyr)
library(forcats)
senate_matrix <- as.matrix(senate)
# Matix without the first and second columns
senate_matrix_12 <- senate %>%
select(-(1:2)) %>% as.matrix()
library(tidyverse)
library(ggplot2)
library(tidyr)
library(readr)
library(purrr)
library(dplyr)
library(forcats)
select <- dplyr::select
senate_matrix <- as.matrix(senate)
# Matix without the first and second columns
senate_matrix_12 <- senate %>%
select(-(1:2)) %>% as.matrix()
senate_norm <- senate_matrix_12
for(z in 1:nrow(senate_norm)){senate_norm[z, ] <- (senate_norm[z, ]/sum(senate_norm[z, ]))}
# K-Means with clusters varies from 2 to N-1
ss <- c()
num_cluster <- c()
# Since it takes so long to run the graph, I limited the obserbations to 120
for (i in 2:120){
k_cluster <- kmeans(senate_norm, centers = i)
num_cluster[i] <- i
ss[i] <- k_cluster$tot.withinss
}
plot(num_cluster, ss)
# K-Means with 6 clusters
n.clust_6 <- 6
set.seed(999)
k_cluster_6 <- kmeans(senate_norm, centers = n.clust_6)
table(k_cluster_6$cluster)
k_cluster_6$cluster
senate
senate_matrix_12
senate_matrix_12[1,]
senate_matrix <- as.matrix(senate)
senate_matrix_12 <- senate %>%
select(-(1:2))
senate_matrix_12 <- senate_matrix %>%
select(-(1:2))
senate_matrix_12 <- senate %>%
select(-(1:2))%>% as.matrix()
senate_matrix_12
subsets <- subset(senate, select = -c(1,2))
mat_sub <- as.matrix(subsets)
senate_norm <- mat_sub
for(z in 1:nrow(senate_norm)){senate_norm[z, ] <- (senate_norm[z, ]/sum(senate_norm[z, ]))}
ss <- c()
num_cluster <- c()
n.clust_6 <- 6
set.seed(999)
k_cluster_6 <- kmeans(senate_norm, centers = n.clust_6)
table(k_cluster_6$cluster)
k_cluster_6$cluster
study <- c(1:4)
Jin <- 1
Tae <- 2
Minju <- 3
Ki <- 4
Robincoffee <- sample(study, 3)
Robincoffee
Hyunkucoffee
Hyunkucoffee <- sample(study, 3)
Hyunkucoffee
library(foreign)
library(readtext)
library(stringr)
library(stm)
library(stats)
strings_df <- readtext("C:/Users/minju/Dropbox/UChicago/MachineLearning/FinalProject/Data/PE2008/*")
strings_df$doc_id <- str_extract(strings_df$doc_id, "\\D{4,}")
processed <- textProcessor(strings_df$text,
removestopwords = T, removenumbers = T,
lowercase = T, removepunctuation = T)
out <- prepDocuments(processed$documents, processed$vocab,
processed$meta)
fit_10 <- stm(out$documents, out$vocab
, K=10
, emtol = 0.00001
, seed = 13579
, verbose = F)
stm_10_labels_10 <- labelTopics(fit_10, n = 10, topics = 1:10)
stm_10_labels_10$frex
stm_10_labels_10 <- labelTopics(fit_10, n = 10, topics = 1:10)
stm_10_labels_10$frex
knitr::opts_chunk$set(echo = TRUE)
library(foreign)
library(readtext)
library(stringr)
library(stm)
library(stats)
library(foreign)
library(readtext)
library(stringr)
library(stm)
library(stats)
library(stats)
rm(list=ls())
library(readr)
library(tidyr)
library(stringr)
library(lubridate)
library(ggmap)
library(magrittr)
library(MASS)
library(dplyr)
library(stargazer)
library(plm)
library(zeligverse)
library(ggplot2)
library(pscl)
library(arm)
library(fBasics, warn.conflicts = FALSE)
library(foreign)
library(readtext)
library(stringr)
library(stm)
library(stats)
library(gpclib)
library(gridExtra)
select <- dplyr::select
sim <- Zelig::sim
logit <- VGAM::logit
expand <- tidyr::expand
setwd("C:/Users/minju/Dropbox/UChicago/MachineLearning/FinalProject")
mac_final <- read.csv("C:/Users/minju/Documents/GitHub/ML_finalPJT/Data/alldf_180223.csv", header=TRUE)
mac_final$Year <- as.numeric(mac_final$Year)
mac_final <-mac_final%>%mutate(swing = ifelse((Year==2008&swing_last_08 <= 0.05)|
(Year==2012&swing_last_12 <= 0.05)|
(Year==2016&swing_last_16 <= 0.05), 1, 0))
mac_final <- mac_final%>%mutate(swing16= ifelse((Year==2016&swing_last_16 <= 0.05), 1, 0))
groupbystate <- mac_final%>%group_by(State)
swinglist <- groupbystate%>%summarise(sum(swing16))
rust <- subset(mac_final, Rustbelt==1)
rb_swing <- subset(mac_final, Rustbelt==1 & swing ==1)
rb_nsw <- subset(mac_final, Rustbelt==1 & swing ==0)
rm(rb_nsw, rb_swing)
rustbelt <- textProcessor(rust$Clean,
removestopwords = T, removenumbers = T,
lowercase = T, removepunctuation = T)
out <- prepDocuments(rustbelt$documents, rustbelt$vocab,
rustbelt$meta)
fit_15 <- stm(out$documents, out$vocab
, K=45
, emtol = 0.00001
, seed = 12345
, verbose = F)
stm_45_labels_15 <- labelTopics(fit_10, n = 15, topics = 1:10)
stm_45_labels_15 <- labelTopics(fit_15, n = 15, topics = 1:10)
result <- stm_45_labels_15$frex
r <- as.data.frame(result)
write.csv(r, file = "C:/Users/minju/Dropbox/UChicago/MachineLearning/FinalProject/rust_15words.csv")
stm_45_labels_10 <- labelTopics(fit_15, n = 10, topics = 1:10)
result <- stm_45_labels_110$frex
result2 <- stm_45_labels_10$frex
r2 <- as.data.frame(result2)
write.csv(r2, file = "C:/Users/minju/Dropbox/UChicago/MachineLearning/FinalProject/rust_10words.csv")
install.packages("wordcloud")
library(wordcloud)
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(RColorBrewer)
library(RColorBrewer)
library(wordcloud)
library(readr)
library(tidyr)
library(stringr)
library(lubridate)
library(ggmap)
library(magrittr)
library(MASS)
library(dplyr)
library(stargazer)
library(plm)
library(zeligverse)
library(ggplot2)
library(pscl)
library(arm)
library(fBasics, warn.conflicts = FALSE)
library(foreign)
library(readtext)
library(stringr)
library(stm)
library(stats)
library(gpclib)
library(gridExtra)
library(RColorBrewer)
library(wordcloud)
cloud(fit_15, topic = 43, scale = c(2,20))
cloud(fit_15, topic = 44, scale = c(2,20))
cloud(fit_15, topic = 44, scale = c(2, 0.25))
rb_swing <- subset(mac_final, Rustbelt==1 & swing ==1)
rb_nsw <- subset(mac_final, Rustbelt==1 & swing ==0)
rust_nonsw <- textProcessor(rb_nsw$Clean,
removestopwords = T, removenumbers = T,
lowercase = T, removepunctuation = T)
out2 <- prepDocuments(rust_nonsw$documents, rust_nonsw$vocab,
rust_nonsw$meta)
nsw_fit_10 <- stm(out2$documents, out2$vocab
, K=45
, emtol = 0.00001
, seed = 12345
, verbose = F)
nsw_stm_45_labels_10 <- labelTopics(nsw_fit_10, n = 10, topics = 1:10)
result <- nsw_stm_45_labels_10$frex
r3 <- as.data.frame(result)
write.csv(r3, file = "C:/Users/minju/Dropbox/UChicago/MachineLearning/FinalProject/rust_nsw.csv")
cloud(nsw_fit_10, topic = 13, scale = c(2, 0.25))
cloud(nsw_fit_10, topic = 13, scale = c(2, 0.25), colors = c("black", "red"),
ordered.colors = FALSE)
cloud(nsw_fit_10, topic = 13, scale = c(2, 0.25), colors = c("black", "red"),
ordered.colors = TRUE)
library(ggplot2)
library(maps)
library(ggmap)
library(gapminder)
library(fiftystater)
library(rgdal)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
library(modelr)
library(forcats)
library(rgeos)
library(maptools)
library(gpclib)
library(gridExtra)
library(purrr)
rm(list= ls())
library(ggplot2)
library(maps)
library(ggmap)
library(gapminder)
library(fiftystater)
library(rgdal)
library(dplyr)
library(tidyr)
library(tidyverse)
library(stringr)
library(modelr)
library(forcats)
library(rgeos)
library(maptools)
library(gpclib)
library(gridExtra)
library(purrr)
setwd("C:/Users/minju/Documents/GitHub/ML_finalPJT")
dtm <- read.csv('Data/alldf_covariates_adddtm.csv')
dtm$sum<-rowSums(dtm[, c(25:34)])/nrow(dtm)
View(dtm)
sumbystate<-aggregate(dtm$sum, by=list(id=dtm$State), FUN=sum)
pro_state <- read.csv('Data/pro_state.csv')
View(pro_state)
propo <- data.frame(state = tolower(rownames(pro_state)), pro_state)
names(propo) <- c("state", "id", "proportion")
map_pro <- merge(fifty_states, propo, by="id",all.x=TRUE)
data("fifty_states")
rust_ex <- c('illinois','pennsylvania', 'west virginia',
'ohio', 'indiana', 'michigan','illinois',
'iowa', 'wisconsin', 'missouri', 'new york')
filter<- fifty_states[fifty_states$id %in% rust_ex,]
p <- ggplot(map_pro, aes(map_id =id)) +
# map points to the fifty_states shape data
geom_map(aes(fill = proportion), map = fifty_states) +
#geom_map(map =filter, aes(col="red", fill=FALSE), fill=NA)  +
geom_map(map = subset(fifty_states, id %in% rust_ex),
fill = NA, colour = "red", size = 1, alpha = 0.2) +
expand_limits(x = fifty_states$long, y = fifty_states$lat) +
coord_map() +
scale_fill_gradient(low="white", high="orange", name="Proportion")+
scale_x_continuous(breaks = NULL) +
scale_y_continuous(breaks = NULL) +
labs(x = "", y = "") +
theme(legend.position = "bottom")+
theme(panel.background = element_rect(fill = 'skyblue')) +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank())
p
p + fifty_states_inset_boxes()
p <- ggplot(map_pro, aes(map_id =id)) +
# map points to the fifty_states shape data
geom_map(aes(fill = proportion), map = fifty_states) +
#geom_map(map =filter, aes(col="red", fill=FALSE), fill=NA)  +
geom_map(map = subset(fifty_states, id %in% rust_ex),
fill = NA, colour = "red", size = 1, alpha = 0.2) +
expand_limits(x = fifty_states$long, y = fifty_states$lat) +
coord_map() +
scale_fill_gradient(low="white", high="orange", name="Proportion")+
scale_x_continuous(breaks = NULL) +
scale_y_continuous(breaks = NULL) +
labs(x = "", y = "") +
theme(legend.position = "bottom")+
theme(panel.background = element_rect(fill = 'skyblue')) +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank()) +
ggtitle("Trade-word Frequency Map in the US Presidential Election Speeches, 2018-2016")
p
p <- ggplot(map_pro, aes(map_id =id)) +
# map points to the fifty_states shape data
geom_map(aes(fill = proportion), map = fifty_states) +
#geom_map(map =filter, aes(col="red", fill=FALSE), fill=NA)  +
geom_map(map = subset(fifty_states, id %in% rust_ex),
fill = NA, colour = "red", size = 1, alpha = 0.2) +
expand_limits(x = fifty_states$long, y = fifty_states$lat) +
coord_map() +
scale_fill_gradient(low="white", high="orange", name="Proportion")+
scale_x_continuous(breaks = NULL) +
scale_y_continuous(breaks = NULL) +
labs(x = "", y = "") +
theme(legend.position = "bottom")+
theme(panel.background = element_rect(fill = 'skyblue')) +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank()) +
ggtitle("Trade-word Frequency Map in the US Presidential Election Speeches")
p
p <- ggplot(map_pro, aes(map_id =id)) +
# map points to the fifty_states shape data
geom_map(aes(fill = proportion), map = fifty_states) +
#geom_map(map =filter, aes(col="red", fill=FALSE), fill=NA)  +
geom_map(map = subset(fifty_states, id %in% rust_ex),
fill = NA, colour = "red", size = 1, alpha = 0.2) +
expand_limits(x = fifty_states$long, y = fifty_states$lat) +
coord_map() +
scale_fill_gradient(low="white", high="orange", name="Proportion")+
scale_x_continuous(breaks = NULL) +
scale_y_continuous(breaks = NULL) +
labs(x = "", y = "") +
theme(legend.position = "bottom")+
theme(panel.background = element_rect(fill = 'skyblue')) +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank()) +
ggtitle("Trade-word Frequency Map in the US Presidential Election Speeches, 2008 - 2016")
p
p + fifty_states_inset_boxes()
p <- ggplot(map_pro, aes(map_id =id)) +
# map points to the fifty_states shape data
geom_map(aes(fill = proportion), map = fifty_states) +
#geom_map(map =filter, aes(col="red", fill=FALSE), fill=NA)  +
geom_map(map = subset(fifty_states, id %in% rust_ex),
fill = NA, colour = "red", size = 1, alpha = 0.2) +
expand_limits(x = fifty_states$long, y = fifty_states$lat) +
coord_map() +
scale_fill_gradient(low="white", high="orange", name="Proportion")+
scale_x_continuous(breaks = NULL) +
scale_y_continuous(breaks = NULL) +
labs(x = "", y = "") +
# theme(legend.position = "bottom")+
theme(panel.background = element_rect(fill = 'skyblue')) +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank(),
axis.title.y=element_blank(),
axis.text.y=element_blank(),
axis.ticks.y=element_blank()) +
ggtitle("Trade-word Frequency Map in the US Presidential Election Speeches, 2008 - 2016")
p
View(map_pro)
View(fifty_states)
